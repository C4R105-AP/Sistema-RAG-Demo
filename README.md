# ğŸ¤– Sistema RAG con FAISS y LLMs

Sistema de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) que permite hacer preguntas sobre documentos tÃ©cnicos utilizando embeddings vectoriales y modelos de lenguaje.

## ğŸš€ Descarga RÃ¡pida (Windows)

**[ğŸ“¥ Descargar Sistema_RAG_Portable.zip](../../releases/latest)**

### Requisitos
- Windows 10/11 (64-bit)
- **Â¡NO requiere Python ni instalaciones!**

### Â¿CÃ³mo usar?
1. Descomprime `Sistema_RAG_Portable.zip`
2. Doble clic en `Sistema_RAG.exe`
3. Se abrirÃ¡ tu navegador automÃ¡ticamente
4. Â¡Haz preguntas sobre tus documentos!

---

## ğŸ¯ Â¿QuÃ© hace este sistema?

Este sistema implementa RAG (Retrieval-Augmented Generation), una tÃ©cnica que:

1. **Vectoriza** tus documentos PDF en una base de datos semÃ¡ntica
2. **Busca** los fragmentos mÃ¡s relevantes cuando haces una pregunta
3. **Genera** una respuesta usando IA basÃ¡ndose en el contexto encontrado

### Ejemplo Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "Â¿QuÃ© parÃ¡metros se usan en IPC-7525A?"               â”‚
â”‚                    (Tu pregunta)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  VectorizaciÃ³n         â”‚
         â”‚  [0.23, -0.45, 0.12...]â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  FAISS busca similares â”‚
         â”‚  Top 4 fragmentos      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LLM genera respuesta  â”‚
         â”‚  (OpenAI/Claude/etc)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "SegÃºn IPC-7525A, los parÃ¡metros principales son..."   â”‚
â”‚                  (Respuesta IA)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Â¿CÃ³mo funciona por dentro? (ExplicaciÃ³n tÃ©cnica)

### 1. Carga y Procesamiento de Documentos

```python
# api_rag.py - LÃ­neas 120-140 (aproximado)

def cargar_documentos():
    """
    Carga PDFs desde uploaded_docs/ y los divide en chunks.
    Cada chunk es un fragmento de ~500 caracteres con overlap
    para mantener contexto entre fragmentos.
    """
    docs_dir = Path("uploaded_docs")
    documentos = []
    
    for pdf_file in docs_dir.glob("*.pdf"):
        loader = PyPDFLoader(str(pdf_file))
        documentos.extend(loader.load())
    
    # Dividir en chunks manejables
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # TamaÃ±o de cada fragmento
        chunk_overlap=50     # Overlap para mantener contexto
    )
    
    return text_splitter.split_documents(documentos)
```

**Â¿Por quÃ© chunks?** Los LLMs tienen lÃ­mite de tokens. Dividir el documento permite:
- Buscar solo las partes relevantes
- Procesar documentos de cualquier tamaÃ±o
- Mantener contexto con el overlap

---

### 2. VectorizaciÃ³n con FAISS

```python
# api_rag.py - LÃ­neas 145-165 (aproximado)

def crear_vectorstore(chunks):
    """
    Convierte texto en vectores matemÃ¡ticos.
    Textos similares tienen vectores cercanos en el espacio.
    
    Ejemplo:
    "stencil design"    â†’ [0.23, -0.45,  0.12, ...]
    "diseÃ±o de stencil" â†’ [0.21, -0.43,  0.14, ...]  â† Similar!
    "perro gato"        â†’ [-0.80, 0.32, -0.67, ...]  â† Diferente!
    """
    
    # Usar embeddings fake para demo (384 dimensiones)
    embeddings = FakeEmbeddings()
    
    # FAISS crea Ã­ndice optimizado para bÃºsqueda rÃ¡pida
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    # Guardar para uso futuro
    vectorstore.save_local("vectorstore_faiss")
    
    return vectorstore
```

**Â¿QuÃ© son embeddings?** Son representaciones numÃ©ricas del significado del texto. Palabras con significado similar tienen vectores similares.

---

### 3. BÃºsqueda SemÃ¡ntica (Retrieval)

```python
# api_rag.py - SimpleRetrievalQA.__call__

def buscar_contexto(pregunta):
    """
    Busca los chunks mÃ¡s relevantes usando similitud coseno.
    
    MatemÃ¡ticamente:
    similitud = cos(Î¸) = (AÂ·B) / (||A|| Ã— ||B||)
    
    Donde A es el vector de la pregunta y B es cada chunk.
    """
    
    # Vectorizar pregunta
    query_vector = embeddings.embed_query(pregunta)
    # query_vector = [0.23, -0.45, 0.12, ...]
    
    # FAISS busca los 4 vectores mÃ¡s cercanos (k=4)
    docs_relevantes = vectorstore.similarity_search(pregunta, k=4)
    
    # Combinar contexto
    contexto = "\n\n".join([doc.page_content for doc in docs_relevantes])
    
    return contexto
```

**Â¿Por quÃ© k=4?** Balance entre:
- Suficiente contexto para responder
- No saturar el lÃ­mite de tokens del LLM
- Tiempo de procesamiento razonable

---

### 4. GeneraciÃ³n con LLM

```python
# api_rag.py - SimpleRetrievalQA.__call__ (continuaciÃ³n)

def generar_respuesta(pregunta, contexto):
    """
    Usa el LLM para generar una respuesta coherente
    basÃ¡ndose SOLO en el contexto proporcionado.
    """
    
    # Crear prompt estructurado
    prompt = f"""Contexto:
{contexto}

Pregunta: {pregunta}

Respuesta (basÃ¡ndote SOLO en el contexto):"""
    
    # Llamar al LLM (OpenAI, Claude, etc.)
    messages = [
        SystemMessage(content="Eres un experto que responde basÃ¡ndote en el contexto."),
        HumanMessage(content=prompt)
    ]
    
    respuesta = llm.invoke(messages)
    
    return respuesta.content
```

**Ventaja del RAG:** El LLM no "inventa" respuestas, se basa en tus documentos reales.

---

### 5. ConfiguraciÃ³n Flexible de LLMs

```python
# api_rag.py - LÃ­neas 200-250 (aproximado)

def inicializar_llm():
    """
    Soporta mÃºltiples LLMs segÃºn variable de entorno LLM_TYPE
    """
    llm_type = os.getenv("LLM_TYPE", "fake").lower()
    
    if llm_type == "openai":
        # GPT-3.5/GPT-4 - RÃ¡pido y preciso
        return ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0,  # 0 = determinista, 1 = creativo
        )
    
    elif llm_type == "anthropic":
        # Claude - Muy bueno con documentos largos
        return ChatAnthropic(
            model="claude-3-sonnet-20240229",
            temperature=0,
        )
    
    elif llm_type == "huggingface":
        # Modelos open-source offline
        return HuggingFacePipeline.from_model_id(
            model_id="microsoft/DialoGPT-medium",
            task="text-generation",
        )
    
    else:
        # Modo fake para demos sin API
        return FakeChatModel()
```

---

## ğŸ› ï¸ Arquitectura Completa

```python
# Flujo completo de una pregunta

async def endpoint_preguntar(pregunta: str):
    """
    /preguntar endpoint - Flujo completo
    """
    
    # 1. Cargar vectorstore
    vectorstore = FAISS.load_local("vectorstore_faiss", embeddings)
    
    # 2. Crear retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    # 3. Crear cadena RAG
    qa_chain = SimpleRetrievalQA(
        llm=llm,           # Modelo de lenguaje
        retriever=retriever # Motor de bÃºsqueda
    )
    
    # 4. Ejecutar consulta
    resultado = qa_chain({"query": pregunta})
    
    # 5. Retornar respuesta + fuentes
    return {
        "respuesta": resultado["result"],
        "fuentes": [doc.page_content[:200] for doc in resultado["source_documents"]]
    }
```

---

## ğŸ”§ ConfiguraciÃ³n Avanzada (Opcional)

### Usar con OpenAI (recomendado para producciÃ³n)

```powershell
# Windows PowerShell
set LLM_TYPE=openai
set OPENAI_API_KEY=tu-api-key-aqui
Sistema_RAG.exe
```

**Obtener API Key:** https://platform.openai.com/api-keys

### ComparaciÃ³n de Costos

| Pregunta tÃ­pica | Tokens | Costo OpenAI (GPT-3.5) |
|----------------|--------|------------------------|
| Simple         | ~500   | $0.0005 (~$0.5/1000)  |
| Con contexto   | ~2000  | $0.002 (~$2/1000)     |
| Compleja       | ~4000  | $0.004 (~$4/1000)     |

---

## ğŸ“Š Documentos Incluidos

- **IPC-7525A - Stencil Design Guidelines** (ejemplo)
- Puedes agregar tus propios documentos copiÃ¡ndolos a la carpeta `uploaded_docs/`

---

## ğŸ¨ Interfaz Web

La interfaz incluida (`interfaz_web.html`) ofrece:

- âœ… DiseÃ±o moderno y responsive
- âœ… Burbujas de chat estilo WhatsApp
- âœ… Muestra documentos fuente usados
- âœ… Indicador de estado del servidor
- âœ… Sin dependencias externas

```html
<!-- interfaz_web.html - Fragmento -->

<script>
async function enviarPregunta() {
    const pregunta = document.getElementById('pregunta').value;
    
    // Llamar a la API
    const response = await fetch('http://localhost:8000/preguntar', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({pregunta, k: 4})
    });
    
    const data = await response.json();
    
    // Mostrar respuesta en chat
    mostrarMensaje(data.respuesta, 'asistente');
    mostrarFuentes(data.source_documents);
}
</script>
```

---

## ğŸ” Seguridad y Privacidad

- âœ… **Procesamiento local:** Tus documentos NO se suben a ningÃºn servidor
- âœ… **API Keys seguras:** Se usan variables de entorno (nunca en cÃ³digo)
- âœ… **Open Source:** Puedes auditar el cÃ³digo completo

**Nota:** Al usar OpenAI/Anthropic, las preguntas SÃ se envÃ­an a sus APIs (no los documentos completos, solo fragmentos relevantes).

---

## ğŸ“ Soporte y Ayuda

### Â¿Problemas con el ejecutable?

1. **Error "Puerto 8000 en uso":**
   - Otro programa estÃ¡ usando el puerto
   - Cierra otras aplicaciones o reinicia

2. **No se abre el navegador:**
   - Abre manualmente: http://localhost:8000

3. **Error 429 (OpenAI):**
   - Sin crÃ©ditos en tu cuenta
   - Agrega crÃ©ditos o usa modo fake

### Â¿Quieres el cÃ³digo fuente?

Este proyecto estÃ¡ disponible como ejecutable standalone. Si eres desarrollador y quieres el cÃ³digo fuente para aprender o modificar, contÃ¡ctame.

---

## ğŸ™ CrÃ©ditos

Construido con:
- [LangChain](https://langchain.com/) - Framework RAG
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search (Meta AI)
- [FastAPI](https://fastapi.tiangolo.com/) - API moderna
- [OpenAI](https://openai.com/) / [Anthropic](https://anthropic.com/) - LLMs

---

## ğŸ“„ Licencia

Este software se proporciona "tal cual" para uso personal y educativo.

---

**â­ Â¿Te resultÃ³ Ãºtil? Â¡Dale una estrella al repositorio!**

**ğŸ’¬ Â¿Preguntas?** Abre un [Issue](../../issues) y te ayudo.
